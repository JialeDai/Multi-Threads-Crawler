The crawler with priority works in the follow step:
- User input key word and specify the number of seeds
- The crawler start the crawler threads and initialize the crawl queue with the seeds url
- The crawl thread consume the crawl queue, get the url with the highest priority, using the url to make a request and down the response html, storing the html to the data queue.
- The parser thread consume the data queue, get the html in the front of the data queue, using xpath expression to extract the url in the href tags. Go through the list of the sub_url in the html, for each url, the crawler check if the url is valid(ambiguity of URLs, Different Types of Files, Checking for Earlier Visits), and if the url is valid, we print it to the update it score (novelty-assume length of the sub-url list is n, for every url we process, we decrease its score by 1/n; importance: for each url in the sub_url list, check if the url is already in the visited set, if yes, then check if the url is still in the crawler queue which means the rul haven't been processed by the crawler, we increment the score of this url by 1) and print is in the log file.


The crawler with bfs version:
- It works similar with the crawler with priority, we use the normal queue instead of a priority queue, and the score combining novelty and importance is not involved.

The concept in the crawler:
crawl_thead(thread_id, queue, file, type_black_list, crawl_mode)
- queue: crawl_queue to process the url, crawl_thead consume the queue and download the html
- file: crawl_log file, used to record every url we visited
- type_black_list: this is the configuration which specify the invalid mime type of the file(text/xml,image/gif,image/jpeg,image/png,application/xhtml+xml,application/xml,application/atom+xml,application/json,application/pdf,application/msword,application/octet-stream,application/x-www-form-urlencoded,multipart/form-data)
- crawl_mode: the crawler give two options 0 for simple bfs strategy, 1 for prioritized crawl.

parser_thread(thread_id, queue, file, crawl_mode, crawl_limit)
- queue: data_queue to process the html response (extract the urls in the html)
- file: data log file, used to record all the url we reached while crawling
- input for option of bfs and prioritized crawl
-crawl_limit is configured in config.ini, specify the number of urls we want to crawl

page_queue_bfs(site, distance + 1)

page_queue(priority, url, distance, parent_url)
- priority is the combination of novelty and importance
- novelty: when we consume html in data_queue, we fetch all the sub url in the html and add the sub url to the page_queue. Every time we process a url in the sub url list, we in decrease the priority by 1/the number of sub url.
- importance: when we are about to add an url to page_queue, we first check if the url is already in the page_queue, if yes, we update the priority score of the url in priority queue by increase it priority by 1.
- parent_url: this is the url from which the crawler get the html. Because for some href tag, it contains incomplete url, like (about.html) we need to know the parent url and call the urllib.join() to make up the complete url.

data_queue

visited
-is a simple dict structure, key is the page url, and the value is times we visited this url, if this url has already been processed in page_queue, we set its value to -1

min_distance
- Originally, distance for all seeds is set to be 0, every time we visited its children, we increment the distance by 1.

flag
- flag is a global variable that control the termination of the crawler. Every time we parser an url, we first check if the crawl_count is equal of larger than the configured crawl_limit, if yes, we set the flag to be true, and the crawler will be ready to terminate

